2023-03-29 16:37:25,983 - INFO - ——————程序运行开始——————
2023-03-29 16:37:25,983 - INFO - ——————————————————————
2023-03-29 16:37:26,026 - INFO - GPU is available
2023-03-29 16:37:26,801 - INFO - 谣言数据总量为：1538
2023-03-29 16:37:26,801 - INFO - 非谣言数据总量为：1849
2023-03-29 16:37:26,813 - INFO - 数据文本生成成功！
2023-03-29 16:37:26,831 - INFO - 成功划分训练集和测试集！
2023-03-29 16:37:36,699 - INFO - ————————————————模型训练开始————————————————
2023-03-29 16:37:37,668 - INFO - Epoch: 0, Batch: 0, Loss: 0.6870942115783691
2023-03-29 16:37:43,261 - INFO - Epoch: 0, Batch: 10, Loss: 0.7233935594558716
2023-03-29 16:37:48,908 - INFO - Epoch: 0, Batch: 20, Loss: 0.6362406611442566
2023-03-29 16:37:54,615 - INFO - Epoch: 0, Batch: 30, Loss: 0.5775843858718872
2023-03-29 16:38:00,294 - INFO - Epoch: 0, Batch: 40, Loss: 0.35301876068115234
2023-03-29 16:38:06,101 - INFO - Epoch: 0, Batch: 50, Loss: 0.3425958752632141
2023-03-29 16:38:11,813 - INFO - Epoch: 0, Batch: 60, Loss: 0.35092467069625854
2023-03-29 16:38:17,549 - INFO - Epoch: 0, Batch: 70, Loss: 0.30135244131088257
2023-03-29 16:38:23,523 - INFO - Epoch: 0, Batch: 80, Loss: 0.3610248565673828
2023-03-29 16:38:29,221 - INFO - Epoch: 0, Batch: 90, Loss: 0.30474233627319336
2023-03-29 16:38:34,989 - INFO - Epoch: 0, Batch: 100, Loss: 0.5185973644256592
2023-03-29 16:38:40,678 - INFO - Epoch: 0, Batch: 110, Loss: 0.3192451596260071
2023-03-29 16:38:46,357 - INFO - Epoch: 0, Batch: 120, Loss: 0.17078161239624023
2023-03-29 16:38:52,037 - INFO - Epoch: 0, Batch: 130, Loss: 0.49671876430511475
2023-03-29 16:38:57,718 - INFO - Epoch: 0, Batch: 140, Loss: 0.3300865590572357
2023-03-29 16:39:03,416 - INFO - Epoch: 0, Batch: 150, Loss: 0.22994545102119446
2023-03-29 16:39:09,093 - INFO - Epoch: 0, Batch: 160, Loss: 0.3578335642814636
2023-03-29 16:39:14,778 - INFO - Epoch: 0, Batch: 170, Loss: 0.2973971962928772
2023-03-29 16:39:20,458 - INFO - Epoch: 0, Batch: 180, Loss: 0.21396157145500183
2023-03-29 16:39:26,144 - INFO - Epoch: 0, Batch: 190, Loss: 0.1443256139755249
2023-03-29 16:39:31,827 - INFO - Epoch: 0, Batch: 200, Loss: 0.42989954352378845
2023-03-29 16:39:37,526 - INFO - Epoch: 0, Batch: 210, Loss: 0.22264204919338226
2023-03-29 16:39:43,226 - INFO - Epoch: 0, Batch: 220, Loss: 0.0661882683634758
2023-03-29 16:39:48,935 - INFO - Epoch: 0, Batch: 230, Loss: 0.23524777591228485
2023-03-29 16:39:54,646 - INFO - Epoch: 0, Batch: 240, Loss: 0.10092075914144516
2023-03-29 16:40:00,347 - INFO - Epoch: 0, Batch: 250, Loss: 0.3978697955608368
2023-03-29 16:40:02,049 - INFO - Epoch [1/3], Average Loss: 0.3741
2023-03-29 16:40:02,623 - INFO - Epoch: 1, Batch: 0, Loss: 0.21625396609306335
2023-03-29 16:40:08,322 - INFO - Epoch: 1, Batch: 10, Loss: 0.19650748372077942
2023-03-29 16:40:14,024 - INFO - Epoch: 1, Batch: 20, Loss: 0.13210299611091614
2023-03-29 16:40:19,729 - INFO - Epoch: 1, Batch: 30, Loss: 0.05522496625781059
2023-03-29 16:40:25,436 - INFO - Epoch: 1, Batch: 40, Loss: 0.2851734459400177
2023-03-29 16:40:31,141 - INFO - Epoch: 1, Batch: 50, Loss: 0.4750322997570038
2023-03-29 16:40:36,863 - INFO - Epoch: 1, Batch: 60, Loss: 0.1297444850206375
2023-03-29 16:40:42,565 - INFO - Epoch: 1, Batch: 70, Loss: 0.02479725331068039
2023-03-29 16:40:48,283 - INFO - Epoch: 1, Batch: 80, Loss: 0.11119124293327332
2023-03-29 16:40:53,982 - INFO - Epoch: 1, Batch: 90, Loss: 0.21999448537826538
2023-03-29 16:40:59,694 - INFO - Epoch: 1, Batch: 100, Loss: 0.24087703227996826
2023-03-29 16:41:05,406 - INFO - Epoch: 1, Batch: 110, Loss: 0.10912444442510605
2023-03-29 16:41:11,115 - INFO - Epoch: 1, Batch: 120, Loss: 0.037255480885505676
2023-03-29 16:41:16,813 - INFO - Epoch: 1, Batch: 130, Loss: 0.30306750535964966
2023-03-29 16:41:22,501 - INFO - Epoch: 1, Batch: 140, Loss: 0.050022102892398834
2023-03-29 16:41:28,206 - INFO - Epoch: 1, Batch: 150, Loss: 0.09314973652362823
2023-03-29 16:41:33,904 - INFO - Epoch: 1, Batch: 160, Loss: 0.26174983382225037
2023-03-29 16:41:39,597 - INFO - Epoch: 1, Batch: 170, Loss: 0.03309093415737152
2023-03-29 16:41:45,301 - INFO - Epoch: 1, Batch: 180, Loss: 0.04720596596598625
2023-03-29 16:41:51,017 - INFO - Epoch: 1, Batch: 190, Loss: 0.049269627779722214
2023-03-29 16:41:56,739 - INFO - Epoch: 1, Batch: 200, Loss: 0.08419271558523178
2023-03-29 16:42:02,495 - INFO - Epoch: 1, Batch: 210, Loss: 0.10353876650333405
2023-03-29 16:42:08,213 - INFO - Epoch: 1, Batch: 220, Loss: 0.01915152557194233
2023-03-29 16:42:13,935 - INFO - Epoch: 1, Batch: 230, Loss: 0.1649312525987625
2023-03-29 16:42:19,644 - INFO - Epoch: 1, Batch: 240, Loss: 0.021951695904135704
2023-03-29 16:42:25,370 - INFO - Epoch: 1, Batch: 250, Loss: 0.17541423439979553
2023-03-29 16:42:27,084 - INFO - Epoch [2/3], Average Loss: 0.1485
2023-03-29 16:42:27,654 - INFO - Epoch: 2, Batch: 0, Loss: 0.118660569190979
2023-03-29 16:42:33,369 - INFO - Epoch: 2, Batch: 10, Loss: 0.064349465072155
2023-03-29 16:42:39,087 - INFO - Epoch: 2, Batch: 20, Loss: 0.018124384805560112
2023-03-29 16:42:44,810 - INFO - Epoch: 2, Batch: 30, Loss: 0.02128755673766136
2023-03-29 16:42:50,531 - INFO - Epoch: 2, Batch: 40, Loss: 0.08738970011472702
2023-03-29 16:42:56,266 - INFO - Epoch: 2, Batch: 50, Loss: 0.02534192055463791
2023-03-29 16:43:01,991 - INFO - Epoch: 2, Batch: 60, Loss: 0.018107546493411064
2023-03-29 16:43:07,712 - INFO - Epoch: 2, Batch: 70, Loss: 0.01240577083081007
2023-03-29 16:43:13,448 - INFO - Epoch: 2, Batch: 80, Loss: 0.038635723292827606
2023-03-29 16:43:19,183 - INFO - Epoch: 2, Batch: 90, Loss: 0.035453490912914276
2023-03-29 16:43:24,936 - INFO - Epoch: 2, Batch: 100, Loss: 0.020630015060305595
2023-03-29 16:43:30,703 - INFO - Epoch: 2, Batch: 110, Loss: 0.021447472274303436
2023-03-29 16:43:36,517 - INFO - Epoch: 2, Batch: 120, Loss: 0.011738961562514305
2023-03-29 16:43:42,243 - INFO - Epoch: 2, Batch: 130, Loss: 0.23811599612236023
2023-03-29 16:43:47,966 - INFO - Epoch: 2, Batch: 140, Loss: 0.026786893606185913
2023-03-29 16:43:53,684 - INFO - Epoch: 2, Batch: 150, Loss: 0.018738089129328728
2023-03-29 16:43:59,416 - INFO - Epoch: 2, Batch: 160, Loss: 0.09909386932849884
2023-03-29 16:44:05,157 - INFO - Epoch: 2, Batch: 170, Loss: 0.011820323765277863
2023-03-29 16:44:10,890 - INFO - Epoch: 2, Batch: 180, Loss: 0.012127473950386047
2023-03-29 16:44:16,627 - INFO - Epoch: 2, Batch: 190, Loss: 0.013939651660621166
2023-03-29 16:44:22,357 - INFO - Epoch: 2, Batch: 200, Loss: 0.0154244564473629
2023-03-29 16:44:28,085 - INFO - Epoch: 2, Batch: 210, Loss: 0.030435895547270775
2023-03-29 16:44:33,805 - INFO - Epoch: 2, Batch: 220, Loss: 0.01874895766377449
2023-03-29 16:44:39,536 - INFO - Epoch: 2, Batch: 230, Loss: 0.03048398718237877
2023-03-29 16:44:45,262 - INFO - Epoch: 2, Batch: 240, Loss: 0.010203070938587189
2023-03-29 16:44:51,005 - INFO - Epoch: 2, Batch: 250, Loss: 0.0408438965678215
2023-03-29 16:44:52,731 - INFO - Epoch [3/3], Average Loss: 0.0551
2023-03-29 16:44:52,731 - INFO - ————————————————模型训练完成————————————————
2023-03-29 16:44:52,731 - INFO - 训练开始时间：2023-03-29 16:37:36
2023-03-29 16:44:52,731 - INFO - 训练结束时间：2023-03-29 16:44:52
2023-03-29 16:44:52,731 - INFO - 模型训练总时间为: 436.03 秒
2023-03-29 16:44:53,506 - INFO - ————————————————模型保存成功————————————————
2023-03-29 16:44:53,998 - INFO - ————————————————模型测试开始————————————————
2023-03-29 16:44:54,192 - INFO - 输入预测标签为：[0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]
2023-03-29 16:44:54,192 - INFO - 输入实际标签为：[0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0]
2023-03-29 16:44:54,389 - INFO - 输入预测标签为：[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]
2023-03-29 16:44:54,389 - INFO - 输入实际标签为：[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]
2023-03-29 16:44:54,582 - INFO - 输入预测标签为：[1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1]
2023-03-29 16:44:54,583 - INFO - 输入实际标签为：[1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1]
2023-03-29 16:44:54,774 - INFO - 输入预测标签为：[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]
2023-03-29 16:44:54,774 - INFO - 输入实际标签为：[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1]
2023-03-29 16:44:54,981 - INFO - 输入预测标签为：[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]
2023-03-29 16:44:54,981 - INFO - 输入实际标签为：[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]
2023-03-29 16:44:55,173 - INFO - 输入预测标签为：[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0]
2023-03-29 16:44:55,173 - INFO - 输入实际标签为：[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0]
2023-03-29 16:44:55,369 - INFO - 输入预测标签为：[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]
2023-03-29 16:44:55,369 - INFO - 输入实际标签为：[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]
2023-03-29 16:44:55,563 - INFO - 输入预测标签为：[0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]
2023-03-29 16:44:55,563 - INFO - 输入实际标签为：[0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]
2023-03-29 16:44:55,755 - INFO - 输入预测标签为：[1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0]
2023-03-29 16:44:55,755 - INFO - 输入实际标签为：[0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0]
2023-03-29 16:44:55,956 - INFO - 输入预测标签为：[1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0]
2023-03-29 16:44:55,956 - INFO - 输入实际标签为：[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0]
2023-03-29 16:44:56,153 - INFO - 输入预测标签为：[1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0]
2023-03-29 16:44:56,153 - INFO - 输入实际标签为：[1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0]
2023-03-29 16:44:56,346 - INFO - 输入预测标签为：[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0]
2023-03-29 16:44:56,347 - INFO - 输入实际标签为：[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0]
2023-03-29 16:44:56,541 - INFO - 输入预测标签为：[1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1]
2023-03-29 16:44:56,542 - INFO - 输入实际标签为：[1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1]
2023-03-29 16:44:56,738 - INFO - 输入预测标签为：[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0]
2023-03-29 16:44:56,738 - INFO - 输入实际标签为：[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0]
2023-03-29 16:44:56,937 - INFO - 输入预测标签为：[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0]
2023-03-29 16:44:56,937 - INFO - 输入实际标签为：[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0]
2023-03-29 16:44:57,134 - INFO - 输入预测标签为：[0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]
2023-03-29 16:44:57,134 - INFO - 输入实际标签为：[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]
2023-03-29 16:44:57,328 - INFO - 输入预测标签为：[0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1]
2023-03-29 16:44:57,329 - INFO - 输入实际标签为：[0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]
2023-03-29 16:44:57,524 - INFO - 输入预测标签为：[0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0]
2023-03-29 16:44:57,524 - INFO - 输入实际标签为：[0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0]
2023-03-29 16:44:57,720 - INFO - 输入预测标签为：[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1]
2023-03-29 16:44:57,720 - INFO - 输入实际标签为：[1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]
2023-03-29 16:44:57,917 - INFO - 输入预测标签为：[1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0]
2023-03-29 16:44:57,917 - INFO - 输入实际标签为：[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0]
2023-03-29 16:44:58,113 - INFO - 输入预测标签为：[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0]
2023-03-29 16:44:58,113 - INFO - 输入实际标签为：[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0]
2023-03-29 16:44:58,310 - INFO - 输入预测标签为：[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0]
2023-03-29 16:44:58,310 - INFO - 输入实际标签为：[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]
2023-03-29 16:44:58,504 - INFO - 输入预测标签为：[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
2023-03-29 16:44:58,504 - INFO - 输入实际标签为：[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
2023-03-29 16:44:58,700 - INFO - 输入预测标签为：[0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]
2023-03-29 16:44:58,701 - INFO - 输入实际标签为：[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0]
2023-03-29 16:44:58,899 - INFO - 输入预测标签为：[1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]
2023-03-29 16:44:58,899 - INFO - 输入实际标签为：[1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]
2023-03-29 16:44:59,095 - INFO - 输入预测标签为：[1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]
2023-03-29 16:44:59,095 - INFO - 输入实际标签为：[1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]
2023-03-29 16:44:59,291 - INFO - 输入预测标签为：[0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]
2023-03-29 16:44:59,291 - INFO - 输入实际标签为：[0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]
2023-03-29 16:44:59,487 - INFO - 输入预测标签为：[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]
2023-03-29 16:44:59,487 - INFO - 输入实际标签为：[0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0]
2023-03-29 16:44:59,487 - INFO - ————————————————模型测试结束————————————————
2023-03-29 16:44:59,487 - INFO - 总测试样本: 336个
2023-03-29 16:44:59,487 - INFO - 预测正确样本: 316个
2023-03-29 16:44:59,487 - INFO - 测试准确度: 0.940476
2023-03-29 16:44:59,488 - INFO - ————————————————预测文本类别————————————————
2023-03-29 16:44:59,515 - INFO - 这条微博有95.15%的概率为非谣言，有4.85%的概率为谣言
2023-03-29 16:44:59,515 - INFO - ——————————————————————
2023-03-29 16:44:59,515 - INFO - ——————程序运行结束——————
