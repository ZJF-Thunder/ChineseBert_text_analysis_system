E:\Anaconda3\envs\pytorch18\python.exe F:/WorkSpace/毕业设计和毕业论文/毕设/data_prossing.py
GPU is available
谣言数据总量为：1538
非谣言数据总量为：1849
数据字典生成成功！
数据列表生成成功！
Some weights of the model checkpoint at chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
F:/WorkSpace/毕业设计和毕业论文/毕设/data_prossing.py:310: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  (torch.tensor(input_ids).cuda(), torch.tensor(token_type_ids).cuda(), torch.tensor(attention_mask).cuda(),
F:/WorkSpace/毕业设计和毕业论文/毕设/data_prossing.py:313: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  (torch.tensor(input_ids).cuda(), torch.tensor(token_type_ids).cuda(), torch.tensor(attention_mask).cuda(),
Epoch: 0, Batch: 10, Loss: 0.6919926404953003
Epoch: 0, Batch: 20, Loss: 0.5571584105491638
Epoch: 0, Batch: 30, Loss: 0.6200810670852661
Epoch: 0, Batch: 40, Loss: 0.5864549875259399
Epoch: 0, Batch: 50, Loss: 0.6958593130111694
Epoch: 0, Batch: 60, Loss: 0.7558039426803589
Epoch: 0, Batch: 70, Loss: 0.755968451499939
Epoch: 0, Batch: 80, Loss: 0.790773868560791
Epoch: 0, Batch: 90, Loss: 0.775898814201355
Epoch: 0, Batch: 100, Loss: 0.5985805988311768
Epoch: 0, Batch: 110, Loss: 0.3069440722465515
Epoch: 0, Batch: 120, Loss: 0.6121841669082642
Epoch: 0, Batch: 130, Loss: 0.669635534286499
Epoch: 0, Batch: 140, Loss: 0.49178922176361084
Epoch: 0, Batch: 150, Loss: 0.6809853315353394
Epoch: 0, Batch: 160, Loss: 0.523871898651123
Epoch: 0, Batch: 170, Loss: 0.61786288022995
Epoch: 0, Batch: 180, Loss: 0.7219933867454529
Epoch: 0, Batch: 190, Loss: 0.768830418586731
Epoch: 0, Batch: 200, Loss: 0.36052143573760986
Epoch: 0, Batch: 210, Loss: 0.6230983138084412
Epoch: 0, Batch: 220, Loss: 0.6375866532325745
Epoch: 0, Batch: 230, Loss: 0.7269784808158875
Epoch: 0, Batch: 240, Loss: 0.6487622261047363
Epoch: 0, Batch: 250, Loss: 0.5563830733299255
Epoch: 0, Batch: 260, Loss: 0.6340271234512329
Epoch: 0, Batch: 270, Loss: 0.7079917192459106
Epoch: 0, Batch: 280, Loss: 0.7277191877365112
Epoch: 0, Batch: 290, Loss: 0.7640984058380127
Epoch: 0, Batch: 300, Loss: 0.7080676555633545
Epoch: 0, Batch: 310, Loss: 0.6721178889274597
Epoch: 0, Batch: 320, Loss: 0.8734131455421448
Epoch: 0, Batch: 330, Loss: 0.7137311697006226
Epoch: 0, Batch: 340, Loss: 0.6834840774536133
Epoch: 0, Batch: 350, Loss: 0.6945455074310303
Epoch: 0, Batch: 360, Loss: 0.7641386389732361
Epoch: 0, Batch: 370, Loss: 0.6941235065460205
Epoch: 0, Batch: 380, Loss: 0.6400648951530457
Epoch: 0, Batch: 390, Loss: 0.6793698668479919
Epoch: 0, Batch: 400, Loss: 0.6211198568344116
Epoch: 0, Batch: 410, Loss: 0.631216287612915
Epoch: 0, Batch: 420, Loss: 0.7821757197380066
Epoch: 0, Batch: 430, Loss: 0.6164536476135254
Epoch: 0, Batch: 440, Loss: 0.7258607745170593
Epoch: 0, Batch: 450, Loss: 0.640904426574707
Epoch: 0, Batch: 460, Loss: 0.5925310254096985
Epoch: 0, Batch: 470, Loss: 0.6333984136581421
Epoch: 0, Batch: 480, Loss: 0.6965513229370117
Epoch: 0, Batch: 490, Loss: 1.0450609922409058
Epoch: 0, Batch: 500, Loss: 0.7382669448852539
Epoch: 0, Batch: 510, Loss: 0.6813631057739258
Epoch: 0, Batch: 520, Loss: 0.6683177351951599
Epoch: 0, Batch: 530, Loss: 0.7017102241516113
Epoch: 0, Batch: 540, Loss: 0.7374647855758667
Epoch: 0, Batch: 550, Loss: 0.685104250907898
Epoch: 0, Batch: 560, Loss: 0.6112823486328125
Epoch: 0, Batch: 570, Loss: 0.7729463577270508
Epoch: 0, Batch: 580, Loss: 0.9700819253921509
Epoch: 0, Batch: 590, Loss: 0.8117648959159851
Epoch: 0, Batch: 600, Loss: 0.6375401020050049
Epoch: 0, Batch: 610, Loss: 0.9174121618270874
Epoch: 0, Batch: 620, Loss: 0.6769540309906006
Epoch: 0, Batch: 630, Loss: 0.7015042901039124
Epoch: 0, Batch: 640, Loss: 0.8415473699569702
Epoch: 0, Batch: 650, Loss: 0.6903619766235352
Epoch: 0, Batch: 660, Loss: 0.7492549419403076
Epoch: 0, Batch: 670, Loss: 0.7588855028152466
Epoch: 0, Batch: 680, Loss: 0.6989381313323975
Epoch: 0, Batch: 690, Loss: 0.5292547345161438
Epoch: 0, Batch: 700, Loss: 0.7514212131500244
Epoch: 0, Batch: 710, Loss: 0.8361700177192688
Epoch: 0, Batch: 720, Loss: 0.702336847782135
Epoch: 0, Batch: 730, Loss: 0.6762632131576538
Epoch: 0, Batch: 740, Loss: 0.7819541096687317
Epoch: 1, Batch: 10, Loss: 0.7514163255691528
Epoch: 1, Batch: 20, Loss: 0.6600357294082642
Epoch: 1, Batch: 30, Loss: 0.8343389630317688
Epoch: 1, Batch: 40, Loss: 0.7186917066574097
Epoch: 1, Batch: 50, Loss: 0.7285637259483337
Epoch: 1, Batch: 60, Loss: 0.6656756401062012
Epoch: 1, Batch: 70, Loss: 0.7090790271759033
Epoch: 1, Batch: 80, Loss: 0.7657579183578491
Epoch: 1, Batch: 90, Loss: 0.6770464777946472
Epoch: 1, Batch: 100, Loss: 0.6247702836990356
Epoch: 1, Batch: 110, Loss: 0.4165602922439575
Epoch: 1, Batch: 120, Loss: 0.6520819664001465
Epoch: 1, Batch: 130, Loss: 0.8433040380477905
Epoch: 1, Batch: 140, Loss: 0.4773658514022827
Epoch: 1, Batch: 150, Loss: 0.8509323596954346
Epoch: 1, Batch: 160, Loss: 0.708919882774353
Epoch: 1, Batch: 170, Loss: 0.7064211964607239
Epoch: 1, Batch: 180, Loss: 0.5951732397079468
Epoch: 1, Batch: 190, Loss: 0.6944864988327026
Epoch: 1, Batch: 200, Loss: 0.5462028384208679
Epoch: 1, Batch: 210, Loss: 0.6764135956764221
Epoch: 1, Batch: 220, Loss: 0.7517994046211243
Epoch: 1, Batch: 230, Loss: 0.7575013637542725
Epoch: 1, Batch: 240, Loss: 0.5615305304527283
Epoch: 1, Batch: 250, Loss: 0.5450418591499329
Epoch: 1, Batch: 260, Loss: 0.5432314276695251
Epoch: 1, Batch: 270, Loss: 0.7624387145042419
Epoch: 1, Batch: 280, Loss: 0.6435320973396301
Epoch: 1, Batch: 290, Loss: 0.8113158941268921
Epoch: 1, Batch: 300, Loss: 0.7317193746566772
Epoch: 1, Batch: 310, Loss: 0.6228460669517517
Epoch: 1, Batch: 320, Loss: 0.6475676894187927
Epoch: 1, Batch: 330, Loss: 0.6142760515213013
Epoch: 1, Batch: 340, Loss: 0.6841239333152771
Epoch: 1, Batch: 350, Loss: 0.7267676591873169
Epoch: 1, Batch: 360, Loss: 0.7035788297653198
Epoch: 1, Batch: 370, Loss: 0.7565870881080627
Epoch: 1, Batch: 380, Loss: 0.652250349521637
Epoch: 1, Batch: 390, Loss: 0.656542181968689
Epoch: 1, Batch: 400, Loss: 0.641615629196167
Epoch: 1, Batch: 410, Loss: 0.759877622127533
Epoch: 1, Batch: 420, Loss: 0.7908639907836914
Epoch: 1, Batch: 430, Loss: 0.6655484437942505
Epoch: 1, Batch: 440, Loss: 0.6953662037849426
Epoch: 1, Batch: 450, Loss: 0.6907016038894653
Epoch: 1, Batch: 460, Loss: 0.5675339102745056
Epoch: 1, Batch: 470, Loss: 0.731593132019043
Epoch: 1, Batch: 480, Loss: 0.7171045541763306
Epoch: 1, Batch: 490, Loss: 1.0526970624923706
Epoch: 1, Batch: 500, Loss: 0.6410365700721741
Epoch: 1, Batch: 510, Loss: 0.7162089347839355
Epoch: 1, Batch: 520, Loss: 0.638418972492218
Epoch: 1, Batch: 530, Loss: 0.6939312219619751
Epoch: 1, Batch: 540, Loss: 0.7629300355911255
Epoch: 1, Batch: 550, Loss: 0.6717966794967651
Epoch: 1, Batch: 560, Loss: 0.7400408983230591
Epoch: 1, Batch: 570, Loss: 0.9193846583366394
Epoch: 1, Batch: 580, Loss: 0.8294134736061096
Epoch: 1, Batch: 590, Loss: 0.777104914188385
Epoch: 1, Batch: 600, Loss: 0.583842933177948
Epoch: 1, Batch: 610, Loss: 0.9020610451698303
Epoch: 1, Batch: 620, Loss: 0.6173095703125
Epoch: 1, Batch: 630, Loss: 0.8285214304924011
Epoch: 1, Batch: 640, Loss: 0.8537673354148865
Epoch: 1, Batch: 650, Loss: 0.7410608530044556
Epoch: 1, Batch: 660, Loss: 0.7068008184432983
Epoch: 1, Batch: 670, Loss: 0.6455681324005127
Epoch: 1, Batch: 680, Loss: 0.6688968539237976
Epoch: 1, Batch: 690, Loss: 0.6482905745506287
Epoch: 1, Batch: 700, Loss: 0.8529702425003052
Epoch: 1, Batch: 710, Loss: 0.7252062559127808
Epoch: 1, Batch: 720, Loss: 0.7996806502342224
Epoch: 1, Batch: 730, Loss: 0.5199177265167236
Epoch: 1, Batch: 740, Loss: 0.5873305797576904
Epoch: 2, Batch: 10, Loss: 0.5535644888877869
Epoch: 2, Batch: 20, Loss: 0.7429267168045044
Epoch: 2, Batch: 30, Loss: 0.6266531944274902
Epoch: 2, Batch: 40, Loss: 0.6797264814376831
Epoch: 2, Batch: 50, Loss: 0.5970466136932373
Epoch: 2, Batch: 60, Loss: 0.6446906924247742
Epoch: 2, Batch: 70, Loss: 0.7025916576385498
Epoch: 2, Batch: 80, Loss: 0.6510211825370789
Epoch: 2, Batch: 90, Loss: 0.6295590400695801
Epoch: 2, Batch: 100, Loss: 0.6569812893867493
Epoch: 2, Batch: 110, Loss: 0.4669696092605591
Epoch: 2, Batch: 120, Loss: 0.6546933054924011
Epoch: 2, Batch: 130, Loss: 0.8818334341049194
Epoch: 2, Batch: 140, Loss: 0.49136531352996826
Epoch: 2, Batch: 150, Loss: 0.8003997802734375
Epoch: 2, Batch: 160, Loss: 0.6551819443702698
Epoch: 2, Batch: 170, Loss: 0.6830450892448425
Epoch: 2, Batch: 180, Loss: 0.6934911608695984
Epoch: 2, Batch: 190, Loss: 0.651757538318634
Epoch: 2, Batch: 200, Loss: 0.5037142038345337
Epoch: 2, Batch: 210, Loss: 0.7973470687866211
Epoch: 2, Batch: 220, Loss: 0.5872484445571899
Epoch: 2, Batch: 230, Loss: 0.7265167236328125
Epoch: 2, Batch: 240, Loss: 0.6486823558807373
Epoch: 2, Batch: 250, Loss: 0.5950719118118286
Epoch: 2, Batch: 260, Loss: 0.5732269287109375
Epoch: 2, Batch: 270, Loss: 0.6510863304138184
Epoch: 2, Batch: 280, Loss: 0.6433180570602417
Epoch: 2, Batch: 290, Loss: 0.6994437575340271
Epoch: 2, Batch: 300, Loss: 0.6435486674308777
Epoch: 2, Batch: 310, Loss: 0.7182506322860718
Epoch: 2, Batch: 320, Loss: 0.63215172290802
Epoch: 2, Batch: 330, Loss: 0.6640521287918091
Epoch: 2, Batch: 340, Loss: 0.7264454364776611
Epoch: 2, Batch: 350, Loss: 0.6608031988143921
Epoch: 2, Batch: 360, Loss: 0.7533681988716125
Epoch: 2, Batch: 370, Loss: 0.7769485712051392
Epoch: 2, Batch: 380, Loss: 0.796478271484375
Epoch: 2, Batch: 390, Loss: 0.6720601320266724
Epoch: 2, Batch: 400, Loss: 0.6241903901100159
Epoch: 2, Batch: 410, Loss: 0.7404999136924744
Epoch: 2, Batch: 420, Loss: 0.7641488313674927
Epoch: 2, Batch: 430, Loss: 0.5264778733253479
Epoch: 2, Batch: 440, Loss: 0.6643602252006531
Epoch: 2, Batch: 450, Loss: 0.7288491725921631
Epoch: 2, Batch: 460, Loss: 0.654302179813385
Epoch: 2, Batch: 470, Loss: 0.6606501936912537
Epoch: 2, Batch: 480, Loss: 0.783159613609314
Epoch: 2, Batch: 490, Loss: 0.993513286113739
Epoch: 2, Batch: 500, Loss: 0.7586727142333984
Epoch: 2, Batch: 510, Loss: 0.7544807195663452
Epoch: 2, Batch: 520, Loss: 0.5590887665748596
Epoch: 2, Batch: 530, Loss: 0.6257801055908203
Epoch: 2, Batch: 540, Loss: 0.5743230581283569
Epoch: 2, Batch: 550, Loss: 0.7302270531654358
Epoch: 2, Batch: 560, Loss: 0.6881128549575806
Epoch: 2, Batch: 570, Loss: 0.8260263800621033
Epoch: 2, Batch: 580, Loss: 0.8552756905555725
Epoch: 2, Batch: 590, Loss: 0.8880265951156616
Epoch: 2, Batch: 600, Loss: 0.7069603204727173
Epoch: 2, Batch: 610, Loss: 0.8298830986022949
Epoch: 2, Batch: 620, Loss: 0.7503313422203064
Epoch: 2, Batch: 630, Loss: 0.7701656818389893
Epoch: 2, Batch: 640, Loss: 0.8115372061729431
Epoch: 2, Batch: 650, Loss: 0.6584497690200806
Epoch: 2, Batch: 660, Loss: 0.7025651931762695
Epoch: 2, Batch: 670, Loss: 0.7766348123550415
Epoch: 2, Batch: 680, Loss: 0.6908661127090454
Epoch: 2, Batch: 690, Loss: 0.6146469116210938
Epoch: 2, Batch: 700, Loss: 0.7873709201812744
Epoch: 2, Batch: 710, Loss: 0.7505500316619873
Epoch: 2, Batch: 720, Loss: 0.7892160415649414
Epoch: 2, Batch: 730, Loss: 0.4764471650123596
Epoch: 2, Batch: 740, Loss: 0.6174324750900269
模型训练总时间为: 969.14 秒
Test accuracy: 0.036057692307692304

进程已结束，退出代码为 0
