E:\Anaconda3\envs\pytorch18\python.exe F:/workspace/毕业设计和毕业论文/毕设/data_prossing.py
GPU is available
谣言数据总量为：1538
非谣言数据总量为：1849
数据字典生成成功！
数据列表生成成功！
Some weights of the model checkpoint at chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
F:/workspace/毕业设计和毕业论文/毕设/data_prossing.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  (torch.tensor(input_ids).cuda(), torch.tensor(token_type_ids).cuda(), torch.tensor(attention_mask).cuda(),
F:/workspace/毕业设计和毕业论文/毕设/data_prossing.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  (torch.tensor(input_ids).cuda(), torch.tensor(token_type_ids).cuda(), torch.tensor(attention_mask).cuda(),
Epoch: 0, Batch: 10, Loss: 0.6972581744194031
Epoch: 0, Batch: 20, Loss: 0.6474223732948303
Epoch: 0, Batch: 30, Loss: 0.7219991087913513
Epoch: 0, Batch: 40, Loss: 0.7868322730064392
Epoch: 0, Batch: 50, Loss: 0.7417430877685547
Epoch: 0, Batch: 60, Loss: 0.618263840675354
Epoch: 0, Batch: 70, Loss: 0.5570417642593384
Epoch: 0, Batch: 80, Loss: 0.8331105709075928
Epoch: 0, Batch: 90, Loss: 0.7752923369407654
Epoch: 0, Batch: 100, Loss: 0.7061094045639038
Epoch: 0, Batch: 110, Loss: 0.8722552061080933
Epoch: 0, Batch: 120, Loss: 0.9386452436447144
Epoch: 0, Batch: 130, Loss: 0.7741011381149292
Epoch: 0, Batch: 140, Loss: 0.6389212608337402
Epoch: 0, Batch: 150, Loss: 0.6387794017791748
Epoch: 0, Batch: 160, Loss: 0.31627026200294495
Epoch: 0, Batch: 170, Loss: 0.6909390687942505
Epoch: 0, Batch: 180, Loss: 0.8808228969573975
Epoch: 0, Batch: 190, Loss: 0.8845866918563843
Epoch: 0, Batch: 200, Loss: 0.6474739909172058
Epoch: 0, Batch: 210, Loss: 0.8078006505966187
Epoch: 0, Batch: 220, Loss: 0.7768212556838989
Epoch: 0, Batch: 230, Loss: 0.7281826138496399
Epoch: 0, Batch: 240, Loss: 0.7874864935874939
Epoch: 0, Batch: 250, Loss: 0.6827113628387451
Epoch: 0, Batch: 260, Loss: 0.6558176279067993
Epoch: 0, Batch: 270, Loss: 0.6732877492904663
Epoch: 0, Batch: 280, Loss: 0.6515860557556152
Epoch: 0, Batch: 290, Loss: 0.5014031529426575
Epoch: 0, Batch: 300, Loss: 0.5861982107162476
Epoch: 0, Batch: 310, Loss: 0.6239094734191895
Epoch: 0, Batch: 320, Loss: 0.7595705986022949
Epoch: 0, Batch: 330, Loss: 0.6675684452056885
Epoch: 0, Batch: 340, Loss: 0.8316843509674072
Epoch: 0, Batch: 350, Loss: 0.6996825933456421
Epoch: 0, Batch: 360, Loss: 0.6425420045852661
Epoch: 0, Batch: 370, Loss: 0.7773194313049316
Epoch: 0, Batch: 380, Loss: 0.7393891215324402
Epoch: 0, Batch: 390, Loss: 0.5728256106376648
Epoch: 0, Batch: 400, Loss: 0.6593910455703735
Epoch: 0, Batch: 410, Loss: 0.5954663753509521
Epoch: 0, Batch: 420, Loss: 0.6086191534996033
Epoch: 0, Batch: 430, Loss: 0.7203054428100586
Epoch: 0, Batch: 440, Loss: 0.7299500703811646
Epoch: 0, Batch: 450, Loss: 0.6418671607971191
Epoch: 0, Batch: 460, Loss: 0.8437466621398926
Epoch: 0, Batch: 470, Loss: 0.6102337837219238
Epoch: 0, Batch: 480, Loss: 0.679236650466919
Epoch: 0, Batch: 490, Loss: 0.6231174468994141
Epoch: 0, Batch: 500, Loss: 0.7501283288002014
Epoch: 0, Batch: 510, Loss: 0.6382780075073242
Epoch: 0, Batch: 520, Loss: 0.6960884928703308
Epoch: 0, Batch: 530, Loss: 0.6677256226539612
Epoch: 0, Batch: 540, Loss: 0.590987503528595
Epoch: 0, Batch: 550, Loss: 0.8115966320037842
Epoch: 0, Batch: 560, Loss: 0.8342292308807373
Epoch: 0, Batch: 570, Loss: 0.8587417006492615
Epoch: 0, Batch: 580, Loss: 0.7024182081222534
Epoch: 0, Batch: 590, Loss: 0.7713010311126709
Epoch: 0, Batch: 600, Loss: 0.7366456985473633
Epoch: 0, Batch: 610, Loss: 0.6833289861679077
Epoch: 0, Batch: 620, Loss: 0.7210115194320679
Epoch: 0, Batch: 630, Loss: 0.8007026314735413
Epoch: 0, Batch: 640, Loss: 0.8102482557296753
Epoch: 0, Batch: 650, Loss: 0.6731947064399719
Epoch: 0, Batch: 660, Loss: 0.7599156498908997
Epoch: 0, Batch: 670, Loss: 0.7032190561294556
Epoch: 0, Batch: 680, Loss: 1.0055122375488281
Epoch: 0, Batch: 690, Loss: 0.7683669328689575
Epoch: 0, Batch: 700, Loss: 0.827896773815155
Epoch: 0, Batch: 710, Loss: 0.7011319994926453
Epoch: 0, Batch: 720, Loss: 0.7139681577682495
Epoch: 0, Batch: 730, Loss: 0.7956326603889465
Epoch: 0, Batch: 740, Loss: 0.6328965425491333
Epoch: 1, Batch: 10, Loss: 0.61479651927948
Epoch: 1, Batch: 20, Loss: 0.6956323385238647
Epoch: 1, Batch: 30, Loss: 0.668502688407898
Epoch: 1, Batch: 40, Loss: 0.6804893016815186
Epoch: 1, Batch: 50, Loss: 0.6733115911483765
Epoch: 1, Batch: 60, Loss: 0.6176308393478394
Epoch: 1, Batch: 70, Loss: 0.69181227684021
Epoch: 1, Batch: 80, Loss: 0.6530282497406006
Epoch: 1, Batch: 90, Loss: 0.7044826149940491
Epoch: 1, Batch: 100, Loss: 0.6285274028778076
Epoch: 1, Batch: 110, Loss: 0.7230260372161865
Epoch: 1, Batch: 120, Loss: 0.7223737835884094
Epoch: 1, Batch: 130, Loss: 0.6823517084121704
Epoch: 1, Batch: 140, Loss: 0.6322098970413208
Epoch: 1, Batch: 150, Loss: 0.5269653797149658
Epoch: 1, Batch: 160, Loss: 0.4130517244338989
Epoch: 1, Batch: 170, Loss: 0.6117371916770935
Epoch: 1, Batch: 180, Loss: 0.8656677007675171
Epoch: 1, Batch: 190, Loss: 0.7800804376602173
Epoch: 1, Batch: 200, Loss: 0.7433105111122131
Epoch: 1, Batch: 210, Loss: 0.6614850759506226
Epoch: 1, Batch: 220, Loss: 0.7589825987815857
Epoch: 1, Batch: 230, Loss: 0.792507529258728
Epoch: 1, Batch: 240, Loss: 0.7652891874313354
Epoch: 1, Batch: 250, Loss: 0.8161388635635376
Epoch: 1, Batch: 260, Loss: 0.6676933765411377
Epoch: 1, Batch: 270, Loss: 0.8176498413085938
Epoch: 1, Batch: 280, Loss: 0.671174943447113
Epoch: 1, Batch: 290, Loss: 0.6270144581794739
Epoch: 1, Batch: 300, Loss: 0.720378577709198
Epoch: 1, Batch: 310, Loss: 0.5343436598777771
Epoch: 1, Batch: 320, Loss: 0.8637328147888184
Epoch: 1, Batch: 330, Loss: 0.6549952626228333
Epoch: 1, Batch: 340, Loss: 0.6743327379226685
Epoch: 1, Batch: 350, Loss: 0.699722945690155
Epoch: 1, Batch: 360, Loss: 0.7141702175140381
Epoch: 1, Batch: 370, Loss: 0.5987048149108887
Epoch: 1, Batch: 380, Loss: 0.6219713687896729
Epoch: 1, Batch: 390, Loss: 0.6668773889541626
Epoch: 1, Batch: 400, Loss: 0.6614850163459778
Epoch: 1, Batch: 410, Loss: 0.6346011757850647
Epoch: 1, Batch: 420, Loss: 0.6709654331207275
Epoch: 1, Batch: 430, Loss: 0.7396635413169861
Epoch: 1, Batch: 440, Loss: 0.6338669061660767
Epoch: 1, Batch: 450, Loss: 0.6624533534049988
Epoch: 1, Batch: 460, Loss: 0.7889848947525024
Epoch: 1, Batch: 470, Loss: 0.6501175761222839
Epoch: 1, Batch: 480, Loss: 0.7637948989868164
Epoch: 1, Batch: 490, Loss: 0.5518209934234619
Epoch: 1, Batch: 500, Loss: 0.7430417537689209
Epoch: 1, Batch: 510, Loss: 0.6010733246803284
Epoch: 1, Batch: 520, Loss: 0.6129072904586792
Epoch: 1, Batch: 530, Loss: 0.6924952268600464
Epoch: 1, Batch: 540, Loss: 0.6372206211090088
Epoch: 1, Batch: 550, Loss: 0.6787418127059937
Epoch: 1, Batch: 560, Loss: 0.8812087178230286
Epoch: 1, Batch: 570, Loss: 0.8393775224685669
Epoch: 1, Batch: 580, Loss: 0.7261433601379395
Epoch: 1, Batch: 590, Loss: 0.6666670441627502
Epoch: 1, Batch: 600, Loss: 0.740820050239563
Epoch: 1, Batch: 610, Loss: 0.6873401999473572
Epoch: 1, Batch: 620, Loss: 0.5772970914840698
Epoch: 1, Batch: 630, Loss: 0.8361800909042358
Epoch: 1, Batch: 640, Loss: 0.8117228746414185
Epoch: 1, Batch: 650, Loss: 0.7112057209014893
Epoch: 1, Batch: 660, Loss: 0.7552231550216675
Epoch: 1, Batch: 670, Loss: 0.6521002650260925
Epoch: 1, Batch: 680, Loss: 0.6652936935424805
Epoch: 1, Batch: 690, Loss: 0.7492926716804504
Epoch: 1, Batch: 700, Loss: 0.8773730397224426
Epoch: 1, Batch: 710, Loss: 0.6793065071105957
Epoch: 1, Batch: 720, Loss: 0.7396355271339417
Epoch: 1, Batch: 730, Loss: 0.7327723503112793
Epoch: 1, Batch: 740, Loss: 0.6056119203567505
Epoch: 2, Batch: 10, Loss: 0.7188243269920349
Epoch: 2, Batch: 20, Loss: 0.8069505095481873
Epoch: 2, Batch: 30, Loss: 0.7740345001220703
Epoch: 2, Batch: 40, Loss: 0.7694921493530273
Epoch: 2, Batch: 50, Loss: 0.6559933423995972
Epoch: 2, Batch: 60, Loss: 0.6901675462722778
Epoch: 2, Batch: 70, Loss: 0.6016043424606323
Epoch: 2, Batch: 80, Loss: 0.5152319669723511
Epoch: 2, Batch: 90, Loss: 0.7284761071205139
Epoch: 2, Batch: 100, Loss: 0.7394949793815613
Epoch: 2, Batch: 110, Loss: 0.800017774105072
Epoch: 2, Batch: 120, Loss: 0.7099239230155945
Epoch: 2, Batch: 130, Loss: 0.666200578212738
Epoch: 2, Batch: 140, Loss: 0.5897437334060669
Epoch: 2, Batch: 150, Loss: 0.5890389084815979
Epoch: 2, Batch: 160, Loss: 0.4250413179397583
Epoch: 2, Batch: 170, Loss: 0.6594930291175842
Epoch: 2, Batch: 180, Loss: 0.8354808688163757
Epoch: 2, Batch: 190, Loss: 0.7211424112319946
Epoch: 2, Batch: 200, Loss: 0.6624340415000916
Epoch: 2, Batch: 210, Loss: 0.798922061920166
Epoch: 2, Batch: 220, Loss: 0.741356372833252
Epoch: 2, Batch: 230, Loss: 0.6906459331512451
Epoch: 2, Batch: 240, Loss: 0.7409406900405884
Epoch: 2, Batch: 250, Loss: 0.730607271194458
Epoch: 2, Batch: 260, Loss: 0.7887473106384277
Epoch: 2, Batch: 270, Loss: 0.7286936044692993
Epoch: 2, Batch: 280, Loss: 0.6807924509048462
Epoch: 2, Batch: 290, Loss: 0.5604708790779114
Epoch: 2, Batch: 300, Loss: 0.5737577676773071
Epoch: 2, Batch: 310, Loss: 0.5705312490463257
Epoch: 2, Batch: 320, Loss: 0.7435764074325562
Epoch: 2, Batch: 330, Loss: 0.653676450252533
Epoch: 2, Batch: 340, Loss: 0.7208950519561768
Epoch: 2, Batch: 350, Loss: 0.5674306750297546
Epoch: 2, Batch: 360, Loss: 0.6426745653152466
Epoch: 2, Batch: 370, Loss: 0.8351789712905884
Epoch: 2, Batch: 380, Loss: 0.6364706158638
Epoch: 2, Batch: 390, Loss: 0.6154789924621582
Epoch: 2, Batch: 400, Loss: 0.6809251308441162
Epoch: 2, Batch: 410, Loss: 0.6493548154830933
Epoch: 2, Batch: 420, Loss: 0.7026378512382507
Epoch: 2, Batch: 430, Loss: 0.6440451145172119
Epoch: 2, Batch: 440, Loss: 0.6694349646568298
Epoch: 2, Batch: 450, Loss: 0.6129857301712036
Epoch: 2, Batch: 460, Loss: 0.8987606167793274
Epoch: 2, Batch: 470, Loss: 0.739643931388855
Epoch: 2, Batch: 480, Loss: 0.7713761329650879
Epoch: 2, Batch: 490, Loss: 0.6312947869300842
Epoch: 2, Batch: 500, Loss: 0.6594743132591248
Epoch: 2, Batch: 510, Loss: 0.6771863698959351
Epoch: 2, Batch: 520, Loss: 0.6878353953361511
Epoch: 2, Batch: 530, Loss: 0.6889785528182983
Epoch: 2, Batch: 540, Loss: 0.8458279371261597
Epoch: 2, Batch: 550, Loss: 0.6552072763442993
Epoch: 2, Batch: 560, Loss: 0.6395591497421265
Epoch: 2, Batch: 570, Loss: 0.9088699221611023
Epoch: 2, Batch: 580, Loss: 0.6761552095413208
Epoch: 2, Batch: 590, Loss: 0.762420117855072
Epoch: 2, Batch: 600, Loss: 0.7244715094566345
Epoch: 2, Batch: 610, Loss: 0.7819194793701172
Epoch: 2, Batch: 620, Loss: 0.5981019735336304
Epoch: 2, Batch: 630, Loss: 0.7171539068222046
Epoch: 2, Batch: 640, Loss: 0.7613008618354797
Epoch: 2, Batch: 650, Loss: 0.6741062998771667
Epoch: 2, Batch: 660, Loss: 0.7471527457237244
Epoch: 2, Batch: 670, Loss: 0.6657559275627136
Epoch: 2, Batch: 680, Loss: 0.684723973274231
Epoch: 2, Batch: 690, Loss: 0.8083340525627136
Epoch: 2, Batch: 700, Loss: 0.7398000955581665
Epoch: 2, Batch: 710, Loss: 0.6449266672134399
Epoch: 2, Batch: 720, Loss: 0.6643053293228149
Epoch: 2, Batch: 730, Loss: 0.8254337310791016
Epoch: 2, Batch: 740, Loss: 0.6778465509414673
Test accuracy: 0.0889423076923077

进程已结束，退出代码为 0
